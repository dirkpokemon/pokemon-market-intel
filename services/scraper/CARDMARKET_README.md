# CardMarket Pokemon Scraper - Production Documentation

## Overview

A production-ready, respectful scraper for CardMarket.com Pokemon products (singles and sealed).

### Key Features

‚úÖ **Respectful Scraping**
- Rate limited (20 requests/minute default)
- Randomized delays (2-5 seconds between requests)
- User-Agent rotation (6 realistic browser agents)
- Graceful error handling

‚úÖ **Comprehensive Data Collection**
- Product name and ID
- Set name and card number
- Price and currency (EUR)
- Condition and language
- Availability/listing count
- Scraped timestamp

‚úÖ **Production-Safe**
- Retry logic with exponential backoff
- Data validation before saving
- Append-only database writes
- Comprehensive logging
- Cron-ready

‚úÖ **EU Proxy Ready**
- Configurable proxy support
- EU-specific configuration
- Easy proxy rotation

## Architecture

```
CardMarket Scraper
‚îú‚îÄ‚îÄ cardmarket_production.py  # Main scraper class
‚îú‚îÄ‚îÄ config_cardmarket.py      # Configuration
‚îú‚îÄ‚îÄ run_cardmarket.py         # Standalone entry point
‚îî‚îÄ‚îÄ utils/
    ‚îú‚îÄ‚îÄ user_agent_rotator.py # UA rotation
    ‚îî‚îÄ‚îÄ delay_manager.py      # Randomized delays
```

## Configuration

### Environment Variables

Create or update `.env`:

```bash
# CardMarket-specific config (prefix: CARDMARKET_)
CARDMARKET_MIN_DELAY_SECONDS=2.0
CARDMARKET_MAX_DELAY_SECONDS=5.0
CARDMARKET_REQUESTS_PER_MINUTE=20
CARDMARKET_MAX_SETS_PER_RUN=5
CARDMARKET_SCRAPE_SINGLES=true
CARDMARKET_SCRAPE_SEALED=true

# Proxy (optional)
CARDMARKET_USE_PROXY=false
CARDMARKET_PROXY_URL=http://your-proxy:port
CARDMARKET_PROXY_COUNTRY=DE
```

### Code Configuration

Edit `app/config_cardmarket.py` for advanced settings:

```python
# Rate limiting
MIN_DELAY_SECONDS: float = 2.0    # Min delay between requests
MAX_DELAY_SECONDS: float = 5.0    # Max delay for randomization
REQUESTS_PER_MINUTE: int = 20     # Rate limit

# Scraping scope
MAX_PAGES_PER_SET: int = 10       # Pages per set
MAX_SETS_PER_RUN: int = 5         # Sets per scrape run

# Priority sets to scrape
PRIORITY_SETS: List[str] = [
    "Base-Set",
    "Scarlet-Violet-151",
    "Paldean-Fates",
    # ... add more
]
```

## Usage

### Run Standalone

```bash
# Direct execution
python run_cardmarket.py

# With Docker
docker compose exec scraper python run_cardmarket.py

# Background execution
docker compose exec -d scraper python run_cardmarket.py
```

### Run with Cron

#### Option 1: System Cron

```bash
# Edit crontab
crontab -e

# Add entry (daily at 3 AM)
0 3 * * * cd /path/to/pokemon-market-intel && docker compose exec -T scraper python run_cardmarket.py >> /var/log/cardmarket.log 2>&1

# Every 6 hours
0 */6 * * * cd /path/to/pokemon-market-intel && docker compose exec -T scraper python run_cardmarket.py >> /var/log/cardmarket.log 2>&1
```

#### Option 2: Integrate with Main Scraper

Edit `services/scraper/app/main.py`:

```python
from app.scrapers.cardmarket_production import CardMarketProductionScraper

# In ScraperService.__init__
if settings.CARDMARKET_ENABLED:
    self.scrapers.append(CardMarketProductionScraper())
```

### Test Run

```bash
# Test without saving
docker compose exec scraper python -c "
import asyncio
from app.scrapers.cardmarket_production import CardMarketProductionScraper
from app.database import init_db

async def test():
    await init_db()
    scraper = CardMarketProductionScraper()
    await scraper.setup_client()
    
    # Test fetching a page (update URL to real one)
    url = 'https://www.cardmarket.com/en/Pokemon'
    html = await scraper.fetch_page(url)
    print(f'Fetched {len(html)} chars')
    
    await scraper.cleanup_client()

asyncio.run(test())
"
```

## Data Collection

### Data Points Collected

For each product (single or sealed):

| Field | Type | Example | Notes |
|-------|------|---------|-------|
| product_name | string | "Charizard ex" | Required |
| product_id | string | "123456" | From URL |
| card_number | string | "006/165" | Singles only |
| set_name | string | "151" | Required |
| category | string | "single" or "sealed" | Required |
| price | decimal | 45.99 | EUR, required |
| currency | string | "EUR" | Always EUR |
| condition | string | "Near Mint" | Normalized |
| language | string | "EN" | 2-letter code |
| country | string | "EU" | Always EU |
| availability | integer | 25 | Listing count |
| seller_name | string | "TopSeller" | If available |
| source_url | string | Full URL | Product page |
| scraped_at | datetime | UTC timestamp | Required |

### Database Schema

Data is saved to the `raw_prices` table (existing schema):

```sql
CREATE TABLE raw_prices (
    id SERIAL PRIMARY KEY,
    card_name VARCHAR(500) NOT NULL,     -- product_name
    card_set VARCHAR(255),                -- set_name
    card_number VARCHAR(100),             -- card_number
    condition VARCHAR(50),                -- condition
    language VARCHAR(10),                 -- language
    price NUMERIC(10, 2) NOT NULL,        -- price
    currency VARCHAR(3) DEFAULT 'EUR',    -- currency
    source VARCHAR(255) NOT NULL,         -- "CardMarket"
    source_url TEXT,                      -- source_url
    seller_name VARCHAR(255),             -- seller_name
    seller_rating NUMERIC(3, 2),          -- (not used)
    stock_quantity INTEGER,               -- availability
    scraped_at TIMESTAMP,                 -- scraped_at
    created_at TIMESTAMP DEFAULT NOW()
);
```

## Scraping Strategy

### Singles (Cards)

1. **Select Priority Sets**: Uses `PRIORITY_SETS` configuration
2. **Build URL**: `/en/Pokemon/Products/Singles/{set-name}`
3. **Fetch Page**: With User-Agent rotation and delays
4. **Parse Products**: Extract card data from HTML
5. **Validate**: Check required fields and price range
6. **Save**: Append to database

### Sealed Products

1. **Target URL**: `/en/Pokemon/Products/Sealed-Products`
2. **Fetch Page**: Same respectful approach
3. **Parse Products**: Extract sealed product data
4. **Identify Set**: Extract from product name
5. **Validate & Save**: Same as singles

### Rate Limiting Strategy

```
Request 1 ‚Üí Wait 2-5s (random) ‚Üí Request 2 ‚Üí Wait 2-5s ‚Üí Request 3
                ‚Üì
        Token bucket enforces 20/minute max
```

## Error Handling

### Retry Logic

```python
@retry_with_backoff(max_retries=3, base_delay=2.0)
async def fetch_page(url):
    # Attempts: 0, then +2s, +4s, +8s
    pass
```

### Graceful Degradation

- HTTP errors ‚Üí Log and skip page
- Parse errors ‚Üí Skip row, continue
- Database errors ‚Üí Rollback, log, re-raise
- Network timeouts ‚Üí Retry with backoff

### Logging Levels

- **INFO**: Major events (start, complete, counts)
- **WARNING**: Retries, missing data
- **ERROR**: Failures, exceptions
- **DEBUG**: Detailed parsing, individual items

## Monitoring

### View Logs

```bash
# Real-time
docker compose logs -f scraper

# Last run
tail -f /tmp/cardmarket_scraper.log

# Errors only
docker compose logs scraper | grep ERROR
```

### Check Database

```sql
-- Recent CardMarket scrapes
SELECT 
    COUNT(*) as total,
    MIN(scraped_at) as first_scrape,
    MAX(scraped_at) as last_scrape,
    AVG(price) as avg_price
FROM raw_prices 
WHERE source = 'CardMarket'
  AND scraped_at > NOW() - INTERVAL '24 hours';

-- Singles vs Sealed
SELECT 
    CASE 
        WHEN card_number IS NOT NULL THEN 'single'
        ELSE 'sealed'
    END as category,
    COUNT(*) as count,
    AVG(price) as avg_price
FROM raw_prices
WHERE source = 'CardMarket'
  AND scraped_at > NOW() - INTERVAL '24 hours'
GROUP BY category;

-- Top sets
SELECT 
    card_set,
    COUNT(*) as count,
    AVG(price) as avg_price
FROM raw_prices
WHERE source = 'CardMarket'
  AND scraped_at > NOW() - INTERVAL '24 hours'
GROUP BY card_set
ORDER BY count DESC
LIMIT 10;
```

## Production Checklist

### Before Going Live

- [ ] **Update URLs**: Verify actual CardMarket URL patterns
- [ ] **Test Selectors**: Inspect HTML and confirm CSS selectors
- [ ] **Configure Proxy**: Set up EU proxy for production
- [ ] **Set Rate Limits**: Conservative values (20 req/min)
- [ ] **Enable Logging**: File logging for production
- [ ] **Test Cron**: Verify cron job executes correctly
- [ ] **Monitor First Run**: Watch logs for issues
- [ ] **Check Data Quality**: Validate saved data
- [ ] **Set Alerts**: Database size, scrape failures
- [ ] **Review ToS**: Ensure compliance with CardMarket's terms

### Production Settings

```bash
# Conservative production settings
CARDMARKET_MIN_DELAY_SECONDS=3.0     # Longer delays
CARDMARKET_MAX_DELAY_SECONDS=8.0     
CARDMARKET_REQUESTS_PER_MINUTE=15    # Lower rate
CARDMARKET_MAX_SETS_PER_RUN=3        # Fewer sets per run
CARDMARKET_USE_PROXY=true            # Enable proxy
```

## Legal & Ethical Considerations

### ‚úÖ What We Do

- Read-only access (no purchases, no login)
- Respectful rate limiting
- Randomized delays
- Proper User-Agent identification
- Public data only
- Append-only storage

### ‚ö†Ô∏è Important Notes

1. **Review CardMarket ToS**: Always check their Terms of Service
2. **Use Official API if Available**: Prefer API over scraping
3. **Don't Overwhelm**: Keep rate limits conservative
4. **Be Transparent**: Identify scraper in User-Agent
5. **Monitor Impact**: Watch for any site performance issues
6. **Respect robots.txt**: Check CardMarket's robots.txt

### Robots.txt Compliance

```bash
# Check CardMarket's robots.txt
curl https://www.cardmarket.com/robots.txt

# Ensure compliance with rules
```

## Troubleshooting

### No Data Scraped

1. **Check URLs**: Verify actual CardMarket URL structure
2. **Inspect HTML**: Use browser dev tools
3. **Update Selectors**: CSS selectors may have changed
4. **Check Logs**: Look for HTTP errors

```bash
docker compose logs scraper | grep "CardMarket"
```

### Rate Limit Errors (429)

1. **Increase Delays**:
```python
MIN_DELAY_SECONDS = 5.0
MAX_DELAY_SECONDS = 10.0
```

2. **Reduce Rate**:
```python
REQUESTS_PER_MINUTE = 10
```

3. **Add Proxy**: Enable EU proxy

### Parsing Errors

1. **Log HTML**: Save problematic pages
```python
with open('/tmp/page.html', 'w') as f:
    f.write(html)
```

2. **Inspect Structure**: Check actual HTML
3. **Update Selectors**: Adjust CSS selectors

### Database Errors

```bash
# Check database connection
docker compose exec postgres psql -U pokemon_user -d pokemon_intel -c "\dt"

# Verify table exists
docker compose exec postgres psql -U pokemon_user -d pokemon_intel -c "SELECT COUNT(*) FROM raw_prices;"
```

## Performance

### Expected Metrics

- **Speed**: ~20 products/minute (rate limited)
- **Memory**: ~300-500 MB
- **CPU**: Low (IO-bound)
- **Duration**: 10-30 minutes per run (depends on scope)
- **Data Volume**: ~500-2000 records per run

### Optimization Tips

1. **Increase concurrency** (if site allows):
```python
CONCURRENT_REQUESTS = 3
```

2. **Use connection pooling** (already implemented):
```python
limits=httpx.Limits(max_keepalive_connections=5)
```

3. **Batch database inserts** (already implemented):
```python
session.add_all(records)  # Bulk insert
```

## Support & Maintenance

### Regular Tasks

- **Weekly**: Review logs for errors
- **Monthly**: Check data quality
- **Quarterly**: Update selectors if needed
- **As Needed**: Adjust rate limits

### Updates

When CardMarket changes their site:

1. Inspect new HTML structure
2. Update CSS selectors in `extract_card_data()` and `extract_sealed_data()`
3. Test on small dataset
4. Deploy update

## Summary

This CardMarket scraper is:

‚úÖ **Production-ready**: Robust error handling, logging  
‚úÖ **Respectful**: Rate limited, delays, proper UA  
‚úÖ **Cron-ready**: Standalone script, exit codes  
‚úÖ **Observable**: Comprehensive logging  
‚úÖ **Maintainable**: Clean code, well-documented  
‚úÖ **Extensible**: Easy to add features  
‚úÖ **EU-focused**: Proxy support, EUR prices  

**Ready for production with proper configuration and monitoring!** üöÄ
